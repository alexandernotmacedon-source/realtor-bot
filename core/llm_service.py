"""
LLM integration service with provider abstraction and fallback chain.

Supports multiple LLM providers (OpenAI, Anthropic) with automatic fallback.
"""
import json
import logging
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, AsyncIterator
from enum import Enum

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

from bot.config import settings, LLMProvider


logger = logging.getLogger(__name__)


class LLMProviderBase(ABC):
    """Abstract base class for LLM providers."""
    
    @abstractmethod
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> str:
        """Generate text response."""
        pass
    
    @abstractmethod
    async def generate_response_stream(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> AsyncIterator[str]:
        """Generate streaming response."""
        pass


class OpenAIProvider(LLMProviderBase):
    """OpenAI provider implementation."""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        """
        Initialize OpenAI provider.
        
        Args:
            api_key: OpenAI API key
            model: Model name to use
        """
        from openai import AsyncOpenAI
        
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        logger.info(f"Initialized OpenAI provider with model {model}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception)
    )
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> str:
        """
        Generate text response from OpenAI.
        
        Args:
            messages: Conversation messages
            system_prompt: Optional system prompt
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text response
        """
        try:
            full_messages = []
            
            if system_prompt:
                full_messages.append({"role": "system", "content": system_prompt})
            
            full_messages.extend(messages)
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=full_messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI generation error: {e}")
            raise
    
    async def generate_response_stream(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> AsyncIterator[str]:
        """
        Generate streaming response from OpenAI.
        
        Args:
            messages: Conversation messages
            system_prompt: Optional system prompt
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Yields:
            Text chunks as they are generated
        """
        try:
            full_messages = []
            
            if system_prompt:
                full_messages.append({"role": "system", "content": system_prompt})
            
            full_messages.extend(messages)
            
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=full_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"OpenAI streaming error: {e}")
            raise


class AnthropicProvider(LLMProviderBase):
    """Anthropic Claude provider implementation."""
    
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
        """
        Initialize Anthropic provider.
        
        Args:
            api_key: Anthropic API key
            model: Model name to use
        """
        from anthropic import AsyncAnthropic
        
        self.client = AsyncAnthropic(api_key=api_key)
        self.model = model
        logger.info(f"Initialized Anthropic provider with model {model}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception)
    )
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> str:
        """
        Generate text response from Anthropic.
        
        Args:
            messages: Conversation messages
            system_prompt: Optional system prompt
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text response
        """
        try:
            response = await self.client.messages.create(
                model=self.model,
                messages=messages,
                system=system_prompt or "",
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropic generation error: {e}")
            raise
    
    async def generate_response_stream(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> AsyncIterator[str]:
        """
        Generate streaming response from Anthropic.
        
        Args:
            messages: Conversation messages
            system_prompt: Optional system prompt
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Yields:
            Text chunks as they are generated
        """
        try:
            async with self.client.messages.stream(
                model=self.model,
                messages=messages,
                system=system_prompt or "",
                temperature=temperature,
                max_tokens=max_tokens
            ) as stream:
                async for text in stream.text_stream:
                    yield text
                    
        except Exception as e:
            logger.error(f"Anthropic streaming error: {e}")
            raise


class LLMService:
    """
    LLM service with provider abstraction and fallback chain.
    
    Automatically handles failures by falling back to alternative providers.
    """
    
    # System prompts
    REALTOR_BOT_SYSTEM_PROMPT = """Ð¢Ñ‹ â€” Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ñ€Ð¸ÐµÐ»Ñ‚Ð¾Ñ€Ð° Ð¿Ð¾ Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð² Ð‘Ð°Ñ‚ÑƒÐ¼Ð¸, Ð“Ñ€ÑƒÐ·Ð¸Ñ.

Ð¢Ð’ÐžÐ¯ Ð—ÐÐ”ÐÐ§Ð:
ÐŸÐ¾Ð¼Ð¾Ñ‡ÑŒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ Ð¿Ð¾Ð´Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½ÑƒÑŽ ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€Ñƒ Ð¸Ð»Ð¸ Ð°Ð¿Ð°Ñ€Ñ‚Ð°Ð¼ÐµÐ½Ñ‚Ñ‹, ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð²Ñ‹ÑÑÐ½Ð¸Ð² ÐµÐ³Ð¾ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð½Ð¾ÑÑ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð´Ð¸Ð°Ð»Ð¾Ð³.

Ð’ÐÐ–ÐÐž Ð¡ÐžÐ‘Ð ÐÐ¢Ð¬ (Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸):
1. Ð‘ÑŽÐ´Ð¶ÐµÑ‚ (Ð² Ð»Ð°Ñ€Ð¸ GEL)
2. Ð–ÐµÐ»Ð°ÐµÐ¼Ð°Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ (Ð² Ð¼Â²)
3. Ð Ð°Ð¹Ð¾Ð½ (Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ Ð‘Ð°Ñ‚ÑƒÐ¼Ð¸, ÐÐ¾Ð²Ñ‹Ð¹ Ð±ÑƒÐ»ÑŒÐ²Ð°Ñ€, ÐœÐ°Ñ…Ð¸Ð½Ð´Ð¶Ð°ÑƒÑ€Ð¸, Ð“Ð¾Ð½Ð¸Ð¾, ÐšÐ¾Ð±ÑƒÐ»ÐµÑ‚Ð¸)
4. ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ¾Ð¼Ð½Ð°Ñ‚ (ÑÑ‚ÑƒÐ´Ð¸Ñ, 1, 2, 3, 4+)
5. Ð¡Ñ‚Ð°Ð´Ð¸Ñ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ (Ð³Ð¾Ñ‚Ð¾Ð²Ð¾Ðµ, ÑÑ‚Ñ€Ð¾ÑÑ‰ÐµÐµÑÑ white frame/black frame, ÐºÐ¾Ñ‚Ð»Ð¾Ð²Ð°Ð½)
6. Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð¶ÐµÐ»Ð°Ð½Ð¸Ñ

ÐšÐžÐÐ¢ÐÐšÐ¢ â€” Ð¢ÐžÐ›Ð¬ÐšÐž ÐŸÐžÐ¡Ð›Ð• ÐŸÐžÐšÐÐ—Ð Ð’ÐÐ Ð˜ÐÐÐ¢ÐžÐ’:
- ÐÐ˜ÐšÐžÐ“Ð”Ð Ð½Ðµ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚ (Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½/Ð²Ñ€ÐµÐ¼Ñ Ð·Ð²Ð¾Ð½ÐºÐ°) Ð´Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð° ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€
- Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐ¾Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð²ÑÐµ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸ Ð¸ ÑÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´Ð±ÐµÑ€Ñ‘Ñ‚Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹
- ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚ Ð·Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»Ðµ Ñ‚Ð¾Ð³Ð¾ ÐºÐ°Ðº ÐºÐ»Ð¸ÐµÐ½Ñ‚ ÑƒÐ²Ð¸Ð´Ð¸Ñ‚ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ

ÐŸÐ ÐÐ’Ð˜Ð›Ð ÐžÐ‘Ð©Ð•ÐÐ˜Ð¯:
- ÐžÐ±Ñ€Ð°Ñ‰Ð°Ð¹ÑÑ Ðº ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ Ð½Ð° "Ð²Ñ‹" (Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾, ÑƒÐ²Ð°Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾)
- Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ñ‚Ñ‘Ð¿Ð»Ñ‹Ð¼ Ð¸ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¼, Ð½Ð¾ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¼
- ÐÐ• Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹Ñ‚Ðµ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚ ÑƒÐ¶Ðµ Ð½Ð°Ð·Ð²Ð°Ð» â€” ÑÑ€Ð°Ð·Ñƒ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ñ‚Ðµ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑƒ
- Ð—Ð°Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, ÐµÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð½ÐµÐ¿Ð¾Ð»Ð½Ð°Ñ
- ÐÐµ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°Ð¹Ñ‚Ðµ Ð²ÑÑ‘ ÑÑ€Ð°Ð·Ñƒ â€” Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³
- Ð•ÑÐ»Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚ ÑÐºÐ°Ð·Ð°Ð» Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾Ðµ â€” ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚Ðµ
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÑÐ¼Ð¾Ð´Ð·Ð¸ ÑƒÐ¼ÐµÑ€ÐµÐ½Ð½Ð¾ Ð´Ð»Ñ Ñ‚Ñ‘Ð¿Ð»Ð¾Ð¹ Ð°Ñ‚Ð¼Ð¾ÑÑ„ÐµÑ€Ñ‹
- ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ Ð¸Ð»Ð¸ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¼ (Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ ÑÐ·Ñ‹ÐºÐ° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°)
- ÐÐ• ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚/Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½/Ð²Ñ€ÐµÐ¼Ñ Ð·Ð²Ð¾Ð½ÐºÐ° â€” ÑÑ‚Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾Ð·Ð¶Ðµ

Ð’ÐÐ›Ð®Ð¢Ð Ð‘Ð®Ð”Ð–Ð•Ð¢Ð:
- Ð•ÑÐ»Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚ ÑƒÐºÐ°Ð·Ð°Ð» ÑÑƒÐ¼Ð¼Ñƒ Ð±ÐµÐ· Ð²Ð°Ð»ÑŽÑ‚Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ "1 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½", "100 Ñ‚Ñ‹ÑÑÑ‡") â€” ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚Ðµ: "Ð­Ñ‚Ð¾ Ð² ÐºÐ°ÐºÐ¾Ð¹ Ð²Ð°Ð»ÑŽÑ‚Ðµ? Ð›Ð°Ñ€Ð¸ (GEL), Ð´Ð¾Ð»Ð»Ð°Ñ€Ñ‹ (USD) Ð¸Ð»Ð¸ Ñ€ÑƒÐ±Ð»Ð¸ (RUB)?"
- ÐšÑƒÑ€Ñ Ð´Ð»Ñ Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð°: 1 USD â‰ˆ 2.7 GEL, 1 USD â‰ˆ 90 RUB

ÐŸÐ•Ð Ð’ÐžÐ• Ð¡ÐžÐžÐ‘Ð©Ð•ÐÐ˜Ð• (ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð²Ð°Ð¶Ð½Ð¾):
ÐšÐ¾Ð³Ð´Ð° ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð¿Ð¸ÑˆÐµÑ‚ Ð² Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ€Ð°Ð·, Ð² ÐžÐ”ÐÐžÐœ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸:
1. ÐŸÐ¾Ð¿Ñ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¾Ñ‚ Ð¸Ð¼ÐµÐ½Ð¸ Ñ€Ð¸ÐµÐ»Ñ‚Ð¾Ñ€Ð° (Ð±ÐµÐ· ÑƒÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¸Ð¼ÐµÐ½Ð¸ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ!")
2. Ð¡ÐºÐ°Ð¶Ð¸Ñ‚Ðµ, Ñ‡Ñ‚Ð¾ Ð²Ñ‹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ñ€Ð¸ÐµÐ»Ñ‚Ð¾Ñ€Ð° Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ÐµÑÑŒ Ð½Ð° Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð² Ð‘Ð°Ñ‚ÑƒÐ¼Ð¸
3. Ð¡Ñ€Ð°Ð·Ñƒ Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾ Ð±ÑŽÐ´Ð¶ÐµÑ‚

ÐŸÑ€Ð¸Ð¼ÐµÑ€: "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð¯ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð²Ð°ÑˆÐµÐ³Ð¾ Ñ€Ð¸ÐµÐ»Ñ‚Ð¾Ñ€Ð° Ð¿Ð¾ Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð² Ð‘Ð°Ñ‚ÑƒÐ¼Ð¸. Ð Ð°Ð´ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ð¿Ð¾Ð´Ð±Ð¾Ñ€Ð¾Ð¼ ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€Ñ‹! ðŸ’« Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð½Ð°Ñ‡Ð½Ñ‘Ð¼ Ñ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð° â€” Ð½Ð° ÐºÐ°ÐºÑƒÑŽ ÑÑƒÐ¼Ð¼Ñƒ Ð²Ñ‹ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚Ðµ Ð¿Ð¾ÐºÑƒÐ¿ÐºÑƒ?"

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ Ð Ð«ÐÐšÐ:
- Ð‘Ð°Ñ‚ÑƒÐ¼Ð¸ â€” Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ð¾Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¸Ð½Ð²ÐµÑÑ‚Ð¸Ñ†Ð¸Ð¹ Ð¸ Ð¶Ð¸Ð·Ð½Ð¸
- Ð¦ÐµÐ½Ñ‹: ÑÑ‚ÑƒÐ´Ð¸Ð¸ Ð¾Ñ‚ 100k Ð»Ð°Ñ€Ð¸, 1-ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ð½Ñ‹Ðµ Ð¾Ñ‚ 150k, 2-ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ð½Ñ‹Ðµ Ð¾Ñ‚ 200k Ð»Ð°Ñ€Ð¸
- ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ñ€Ð°Ð¹Ð¾Ð½Ñ‹: ÐÐ¾Ð²Ñ‹Ð¹ Ð±ÑƒÐ»ÑŒÐ²Ð°Ñ€ (Ð¼Ð¾Ñ€Ðµ), Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ Ð³Ð¾Ñ€Ð¾Ð´ (Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€), ÐœÐ°Ñ…Ð¸Ð½Ð´Ð¶Ð°ÑƒÑ€Ð¸ (ÑÐ¿Ð¾ÐºÐ¾Ð¹ÑÑ‚Ð²Ð¸Ðµ)
- White frame = Ð¿Ð¾Ð´ Ñ‡Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽ Ð¾Ñ‚Ð´ÐµÐ»ÐºÑƒ, Black frame = Ð¿Ð¾Ð´ ÐºÐ»ÑŽÑ‡

ÐšÐžÐ“Ð”Ð Ð’Ð¡Ð¯ Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ Ð¡ÐžÐ‘Ð ÐÐÐ:
Ð¡ÐºÐ°Ð¶Ð¸Ñ‚Ðµ: "ÐžÑ‚Ð»Ð¸Ñ‡Ð½Ð¾! Ð£ Ð¼ÐµÐ½Ñ ÐµÑÑ‚ÑŒ Ð²ÑÑ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ. Ð¡ÐµÐ¹Ñ‡Ð°Ñ Ð¿Ð¾Ð´Ð±ÐµÑ€Ñƒ Ð´Ð»Ñ Ð²Ð°Ñ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð¸ Ð¿Ñ€Ð¸ÑˆÑƒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹." â€” ÐÐ• ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚, ÑÑ‚Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð° ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€.
"""
    
    EXTRACTION_PROMPT = """ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ðµ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON.

Ð’ÐµÑ€Ð½Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON Ð±ÐµÐ· Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹:
{
    "budget": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð¾Ð¼ Ð¸Ð»Ð¸ null",
    "size": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒÑŽ Ð¸Ð»Ð¸ null",
    "location": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ Ñ€Ð°Ð¹Ð¾Ð½Ð¾Ð¼ Ð¸Ð»Ð¸ null",
    "rooms": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ð°Ð¼Ð¸ Ð¸Ð»Ð¸ null",
    "ready_status": "ÑÑ‚Ñ€Ð¾ÐºÐ° ÑÐ¾ ÑÑ‚Ð°Ð´Ð¸ÐµÐ¹ Ð¸Ð»Ð¸ null",
    "contact": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð¼ Ð¸Ð»Ð¸ null",
    "notes": "ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ Ð¿Ð¾Ð¶ÐµÐ»Ð°Ð½Ð¸ÑÐ¼Ð¸ Ð¸Ð»Ð¸ null",
    "is_complete": true/false  // Ð²ÑÐµ Ð»Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ñ‹
}

ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ: budget, size, location, rooms
ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: contact â€” ÐÐ• Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»Ðµ. Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ð¾ÐºÐ°Ð¶Ð¸ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€, Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑÐ¿Ñ€Ð¾ÑÐ¸ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚ ÐµÑÐ»Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð·Ð°Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ¾Ð²Ð°Ð½."""
    
    def __init__(
        self,
        primary_provider: LLMProvider,
        fallback_providers: List[LLMProvider],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 500,
        stream: bool = False
    ):
        """
        Initialize LLM service.
        
        Args:
            primary_provider: Primary LLM provider to use
            fallback_providers: List of fallback providers
            model: Model name to use
            temperature: Generation temperature
            max_tokens: Maximum tokens to generate
            stream: Enable streaming responses
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.stream = stream
        
        # Initialize providers
        self.providers: Dict[LLMProvider, LLMProviderBase] = {}
        
        # Setup primary provider
        self._setup_provider(primary_provider)
        
        # Setup fallback providers
        for provider in fallback_providers:
            self._setup_provider(provider)
        
        self.provider_order = [primary_provider] + fallback_providers
        logger.info(f"Initialized LLM service with provider order: {[p.value for p in self.provider_order]}")
    
    def _setup_provider(self, provider: LLMProvider) -> None:
        """Setup a provider instance."""
        if provider == LLMProvider.OPENAI:
            if settings.openai_api_key:
                self.providers[provider] = OpenAIProvider(
                    settings.openai_api_key,
                    self.model
                )
        elif provider == LLMProvider.ANTHROPIC:
            if settings.anthropic_api_key:
                anthropic_model = os.getenv(
                    "ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"
                )
                self.providers[provider] = AnthropicProvider(
                    settings.anthropic_api_key,
                    anthropic_model,
                )
    
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """
        Generate response with fallback chain.
        
        Args:
            messages: Conversation messages
            system_prompt: Optional system prompt
            
        Returns:
            Generated response or None if all providers fail
        """
        if system_prompt is None:
            system_prompt = self.REALTOR_BOT_SYSTEM_PROMPT
        
        for provider_type in self.provider_order:
            provider = self.providers.get(provider_type)
            
            if not provider:
                logger.warning(f"Provider {provider_type.value} not configured, skipping")
                continue
            
            try:
                logger.info(f"Attempting generation with {provider_type.value}")
                
                response = await provider.generate_response(
                    messages=messages,
                    system_prompt=system_prompt,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                logger.info(f"Successfully generated response with {provider_type.value}")
                return response
                
            except Exception as e:
                logger.error(
                    f"Failed to generate with {provider_type.value}: {e}, "
                    "trying next provider"
                )
                continue
        
        logger.error("All LLM providers failed")
        return None
    
    async def extract_client_info(
        self,
        conversation_history: List[Dict[str, str]]
    ) -> Dict:
        """
        Extract structured client information from conversation.
        
        Args:
            conversation_history: List of conversation messages
            
        Returns:
            Dictionary with extracted client information
        """
        extraction_message = {
            "role": "user",
            "content": f"Ð”Ð¸Ð°Ð»Ð¾Ð³:\n{json.dumps(conversation_history, ensure_ascii=False, indent=2)}"
        }
        
        response = await self.generate_response(
            messages=[extraction_message],
            system_prompt=self.EXTRACTION_PROMPT
        )
        
        if not response:
            return {"is_complete": False}
        
        try:
            # Extract JSON from response (in case there's extra text)
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
            
            return json.loads(response)
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse extraction response: {e}")
            return {"is_complete": False}
    
    async def transcribe_audio(self, audio_path: str) -> Optional[str]:
        """
        Transcribe audio file using Groq Whisper API (free tier).
        Falls back to OpenAI if Groq is not configured.
        
        Args:
            audio_path: Path to audio file
            
        Returns:
            Transcribed text or None if failed
        """
        groq_key = os.getenv("GROQ_API_KEY")
        
        # Try Groq first (free)
        if groq_key:
            try:
                import httpx
                
                async with httpx.AsyncClient(timeout=60.0) as client:
                    with open(audio_path, "rb") as audio_file:
                        files = {"file": ("audio.oga", audio_file, "audio/ogg")}
                        data = {"model": "whisper-large-v3", "language": "ru"}
                        headers = {"Authorization": f"Bearer {groq_key}"}
                        
                        response = await client.post(
                            "https://api.groq.com/openai/v1/audio/transcriptions",
                            headers=headers,
                            files=files,
                            data=data
                        )
                        response.raise_for_status()
                        result = response.json()
                        
                        logger.info("Transcribed audio using Groq (free)")
                        return result.get("text")
                        
            except Exception as e:
                logger.warning(f"Groq transcription failed: {e}, falling back to OpenAI")
        
        # Fallback to OpenAI
        openai_provider = self.providers.get(LLMProvider.OPENAI)
        
        if not openai_provider:
            logger.error("No transcription provider available")
            return None
        
        try:
            with open(audio_path, "rb") as audio_file:
                transcript = await openai_provider.client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file
                )
            
            return transcript.text
            
        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return None


__all__ = ["LLMService", "LLMProviderBase"]
